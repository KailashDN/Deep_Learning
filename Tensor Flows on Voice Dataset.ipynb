{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pds\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv')\n",
    "\n",
    "dataframeX.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(20,), activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 751\n",
      "Trainable params: 751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2534 samples, validate on 634 samples\n",
      "Epoch 1/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2413 - acc: 0.6251 - val_loss: 0.3317 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2351 - acc: 0.6251 - val_loss: 0.3793 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2344 - acc: 0.6251 - val_loss: 0.3923 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2342 - acc: 0.6251 - val_loss: 0.3877 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2341 - acc: 0.6251 - val_loss: 0.3907 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3970 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3933 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2336 - acc: 0.6251 - val_loss: 0.3950 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2333 - acc: 0.6251 - val_loss: 0.3925 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2332 - acc: 0.6251 - val_loss: 0.3843 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2330 - acc: 0.6251 - val_loss: 0.3896 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2326 - acc: 0.6251 - val_loss: 0.3890 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2322 - acc: 0.6251 - val_loss: 0.3947 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2317 - acc: 0.6251 - val_loss: 0.3941 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2312 - acc: 0.6251 - val_loss: 0.3930 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2305 - acc: 0.6251 - val_loss: 0.3967 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2295 - acc: 0.6251 - val_loss: 0.3904 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2285 - acc: 0.6251 - val_loss: 0.3916 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2272 - acc: 0.6251 - val_loss: 0.3865 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2256 - acc: 0.6251 - val_loss: 0.3869 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2237 - acc: 0.6251 - val_loss: 0.3787 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2216 - acc: 0.6251 - val_loss: 0.3874 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2189 - acc: 0.6251 - val_loss: 0.3762 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2160 - acc: 0.6251 - val_loss: 0.3623 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2124 - acc: 0.6251 - val_loss: 0.3482 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2088 - acc: 0.6251 - val_loss: 0.3415 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2047 - acc: 0.6251 - val_loss: 0.3430 - val_acc: 0.0410\n",
      "Epoch 28/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.2006 - acc: 0.6555 - val_loss: 0.3230 - val_acc: 0.2886\n",
      "Epoch 29/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1962 - acc: 0.7226 - val_loss: 0.3245 - val_acc: 0.3517\n",
      "Epoch 30/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1917 - acc: 0.7514 - val_loss: 0.3064 - val_acc: 0.5032\n",
      "Epoch 31/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1875 - acc: 0.7723 - val_loss: 0.2979 - val_acc: 0.5568\n",
      "Epoch 32/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1832 - acc: 0.7853 - val_loss: 0.2826 - val_acc: 0.6199\n",
      "Epoch 33/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1785 - acc: 0.7948 - val_loss: 0.2905 - val_acc: 0.5994\n",
      "Epoch 34/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1750 - acc: 0.8031 - val_loss: 0.2778 - val_acc: 0.6309\n",
      "Epoch 35/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1709 - acc: 0.8058 - val_loss: 0.2570 - val_acc: 0.6751\n",
      "Epoch 36/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1676 - acc: 0.8058 - val_loss: 0.2659 - val_acc: 0.6546\n",
      "Epoch 37/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1634 - acc: 0.8114 - val_loss: 0.2483 - val_acc: 0.6909\n",
      "Epoch 38/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1601 - acc: 0.8133 - val_loss: 0.2405 - val_acc: 0.6972\n",
      "Epoch 39/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1568 - acc: 0.8208 - val_loss: 0.2527 - val_acc: 0.6751\n",
      "Epoch 40/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1540 - acc: 0.8220 - val_loss: 0.2310 - val_acc: 0.7161\n",
      "Epoch 41/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1511 - acc: 0.8260 - val_loss: 0.2315 - val_acc: 0.7129\n",
      "Epoch 42/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1477 - acc: 0.8335 - val_loss: 0.2176 - val_acc: 0.7287\n",
      "Epoch 43/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1457 - acc: 0.8323 - val_loss: 0.2272 - val_acc: 0.7129\n",
      "Epoch 44/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1427 - acc: 0.8386 - val_loss: 0.2223 - val_acc: 0.72080.84\n",
      "Epoch 45/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1404 - acc: 0.8394 - val_loss: 0.2033 - val_acc: 0.7429\n",
      "Epoch 46/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1385 - acc: 0.8433 - val_loss: 0.1952 - val_acc: 0.7587\n",
      "Epoch 47/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1360 - acc: 0.8425 - val_loss: 0.1990 - val_acc: 0.7524\n",
      "Epoch 48/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1342 - acc: 0.8477 - val_loss: 0.2100 - val_acc: 0.7334\n",
      "Epoch 49/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1324 - acc: 0.8453 - val_loss: 0.1941 - val_acc: 0.7571\n",
      "Epoch 50/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1301 - acc: 0.8493 - val_loss: 0.1880 - val_acc: 0.7618\n",
      "Epoch 51/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1285 - acc: 0.8516 - val_loss: 0.1931 - val_acc: 0.7539\n",
      "Epoch 52/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1265 - acc: 0.8504 - val_loss: 0.1767 - val_acc: 0.7729\n",
      "Epoch 53/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1256 - acc: 0.8540 - val_loss: 0.1862 - val_acc: 0.7650\n",
      "Epoch 54/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1237 - acc: 0.8564 - val_loss: 0.1821 - val_acc: 0.7650\n",
      "Epoch 55/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1221 - acc: 0.8571 - val_loss: 0.1912 - val_acc: 0.7539\n",
      "Epoch 56/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1207 - acc: 0.8591 - val_loss: 0.1758 - val_acc: 0.7681\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.1189 - acc: 0.8619 - val_loss: 0.1649 - val_acc: 0.7792\n",
      "Epoch 58/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1177 - acc: 0.8639 - val_loss: 0.1674 - val_acc: 0.7744\n",
      "Epoch 59/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1174 - acc: 0.8635 - val_loss: 0.1815 - val_acc: 0.7618\n",
      "Epoch 60/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1156 - acc: 0.8662 - val_loss: 0.1558 - val_acc: 0.7902\n",
      "Epoch 61/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1145 - acc: 0.8674 - val_loss: 0.1662 - val_acc: 0.7744\n",
      "Epoch 62/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1131 - acc: 0.8694 - val_loss: 0.1735 - val_acc: 0.7650\n",
      "Epoch 63/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1117 - acc: 0.8682 - val_loss: 0.1613 - val_acc: 0.7823\n",
      "Epoch 64/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1100 - acc: 0.8757 - val_loss: 0.1549 - val_acc: 0.7918\n",
      "Epoch 65/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1097 - acc: 0.8717 - val_loss: 0.1685 - val_acc: 0.7744\n",
      "Epoch 66/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1079 - acc: 0.8741 - val_loss: 0.1559 - val_acc: 0.7902\n",
      "Epoch 67/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1073 - acc: 0.8781 - val_loss: 0.1583 - val_acc: 0.7855\n",
      "Epoch 68/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1060 - acc: 0.8800 - val_loss: 0.1681 - val_acc: 0.7760\n",
      "Epoch 69/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1045 - acc: 0.8816 - val_loss: 0.1502 - val_acc: 0.8028\n",
      "Epoch 70/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1040 - acc: 0.8792 - val_loss: 0.1588 - val_acc: 0.7871\n",
      "Epoch 71/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1032 - acc: 0.8812 - val_loss: 0.1446 - val_acc: 0.7997\n",
      "Epoch 72/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1024 - acc: 0.8800 - val_loss: 0.1719 - val_acc: 0.7697\n",
      "Epoch 73/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1016 - acc: 0.8832 - val_loss: 0.1346 - val_acc: 0.8202\n",
      "Epoch 74/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1007 - acc: 0.8863 - val_loss: 0.1688 - val_acc: 0.7744\n",
      "Epoch 75/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.1001 - acc: 0.8860 - val_loss: 0.1479 - val_acc: 0.7965\n",
      "Epoch 76/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0988 - acc: 0.8863 - val_loss: 0.1252 - val_acc: 0.8281\n",
      "Epoch 77/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0991 - acc: 0.8879 - val_loss: 0.1642 - val_acc: 0.7760\n",
      "Epoch 78/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0971 - acc: 0.8887 - val_loss: 0.1420 - val_acc: 0.8044\n",
      "Epoch 79/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0966 - acc: 0.8919 - val_loss: 0.1335 - val_acc: 0.81390.89\n",
      "Epoch 80/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0952 - acc: 0.8903 - val_loss: 0.1288 - val_acc: 0.8249\n",
      "Epoch 81/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0948 - acc: 0.8942 - val_loss: 0.1758 - val_acc: 0.7650\n",
      "Epoch 82/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0944 - acc: 0.8911 - val_loss: 0.1476 - val_acc: 0.7950\n",
      "Epoch 83/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0933 - acc: 0.8938 - val_loss: 0.1214 - val_acc: 0.8375\n",
      "Epoch 84/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0926 - acc: 0.8962 - val_loss: 0.1276 - val_acc: 0.8281\n",
      "Epoch 85/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0919 - acc: 0.8954 - val_loss: 0.1169 - val_acc: 0.8438\n",
      "Epoch 86/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0917 - acc: 0.8950 - val_loss: 0.1349 - val_acc: 0.8139\n",
      "Epoch 87/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0907 - acc: 0.8966 - val_loss: 0.1431 - val_acc: 0.7981\n",
      "Epoch 88/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0912 - acc: 0.8942 - val_loss: 0.1164 - val_acc: 0.8454\n",
      "Epoch 89/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0892 - acc: 0.8998 - val_loss: 0.1303 - val_acc: 0.8233\n",
      "Epoch 90/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0888 - acc: 0.9002 - val_loss: 0.1264 - val_acc: 0.8297\n",
      "Epoch 91/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0880 - acc: 0.9002 - val_loss: 0.1332 - val_acc: 0.8186\n",
      "Epoch 92/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0873 - acc: 0.9009 - val_loss: 0.1324 - val_acc: 0.8186\n",
      "Epoch 93/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0871 - acc: 0.9002 - val_loss: 0.1251 - val_acc: 0.8344\n",
      "Epoch 94/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0860 - acc: 0.9037 - val_loss: 0.1196 - val_acc: 0.8391\n",
      "Epoch 95/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0860 - acc: 0.9045 - val_loss: 0.1260 - val_acc: 0.8328\n",
      "Epoch 96/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0853 - acc: 0.9041 - val_loss: 0.1491 - val_acc: 0.8013\n",
      "Epoch 97/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0851 - acc: 0.9049 - val_loss: 0.1263 - val_acc: 0.8328\n",
      "Epoch 98/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0847 - acc: 0.9065 - val_loss: 0.1131 - val_acc: 0.8470\n",
      "Epoch 99/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0851 - acc: 0.9033 - val_loss: 0.1194 - val_acc: 0.8375\n",
      "Epoch 100/100\n",
      "2534/2534 [==============================] - 0s - loss: 0.0834 - acc: 0.9092 - val_loss: 0.1244 - val_acc: 0.8312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11be62a58>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(20,), init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10,init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(10, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=100, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the Input Columns,increasing the number of Neurons and increasing the Epoch Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       IQR    sp.ent       sfm      mode  \\\n",
      "0  0.059781  0.064241  0.032027  0.075122  0.893369  0.491918  0.000000   \n",
      "1  0.066009  0.067310  0.040229  0.073252  0.892193  0.513724  0.000000   \n",
      "2  0.077316  0.083829  0.036718  0.123207  0.846389  0.478905  0.000000   \n",
      "3  0.151228  0.072111  0.158011  0.111374  0.963322  0.727232  0.083878   \n",
      "4  0.135120  0.079146  0.124656  0.127325  0.971955  0.783568  0.104261   \n",
      "\n",
      "   centroid   meanfun   dfrange   modindx  \n",
      "0  0.059781  0.084279  0.000000  0.000000  \n",
      "1  0.066009  0.107937  0.046875  0.052632  \n",
      "2  0.077316  0.098706  0.007812  0.046512  \n",
      "3  0.151228  0.088965  0.554688  0.247119  \n",
      "4  0.135120  0.106398  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,5,8,9,10,11,12,18,19])\n",
    "print(dataframeX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3168, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframeY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(11, input_shape=(11,), activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(11, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(11, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 408\n",
      "Trainable params: 408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2534 samples, validate on 634 samples\n",
      "Epoch 1/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2433 - acc: 0.6235 - val_loss: 0.3204 - val_acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2354 - acc: 0.6251 - val_loss: 0.3725 - val_acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2345 - acc: 0.6251 - val_loss: 0.3913 - val_acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3890 - val_acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2344 - acc: 0.6251 - val_loss: 0.3924 - val_acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3924 - val_acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2342 - acc: 0.6251 - val_loss: 0.3921 - val_acc: 0.0000e+00\n",
      "Epoch 8/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2342 - acc: 0.6251 - val_loss: 0.3925 - val_acc: 0.0000e+00\n",
      "Epoch 9/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2341 - acc: 0.6251 - val_loss: 0.3896 - val_acc: 0.0000e+00\n",
      "Epoch 10/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2340 - acc: 0.6251 - val_loss: 0.3952 - val_acc: 0.0000e+00\n",
      "Epoch 11/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3909 - val_acc: 0.0000e+00\n",
      "Epoch 12/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2339 - acc: 0.6251 - val_loss: 0.3925 - val_acc: 0.0000e+00\n",
      "Epoch 13/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2338 - acc: 0.6251 - val_loss: 0.3889 - val_acc: 0.0000e+00\n",
      "Epoch 14/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2338 - acc: 0.6251 - val_loss: 0.3930 - val_acc: 0.0000e+00\n",
      "Epoch 15/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2336 - acc: 0.6251 - val_loss: 0.3848 - val_acc: 0.0000e+00\n",
      "Epoch 16/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2335 - acc: 0.6251 - val_loss: 0.3938 - val_acc: 0.0000e+00\n",
      "Epoch 17/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2335 - acc: 0.6251 - val_loss: 0.3908 - val_acc: 0.0000e+00\n",
      "Epoch 18/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2333 - acc: 0.6251 - val_loss: 0.3904 - val_acc: 0.0000e+00\n",
      "Epoch 19/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2332 - acc: 0.6251 - val_loss: 0.3849 - val_acc: 0.0000e+00\n",
      "Epoch 20/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2331 - acc: 0.6251 - val_loss: 0.3906 - val_acc: 0.0000e+00\n",
      "Epoch 21/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2330 - acc: 0.6251 - val_loss: 0.3976 - val_acc: 0.0000e+00\n",
      "Epoch 22/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2326 - acc: 0.6251 - val_loss: 0.3869 - val_acc: 0.0000e+00\n",
      "Epoch 23/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2325 - acc: 0.6251 - val_loss: 0.3895 - val_acc: 0.0000e+00\n",
      "Epoch 24/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2324 - acc: 0.6251 - val_loss: 0.3862 - val_acc: 0.0000e+00\n",
      "Epoch 25/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2320 - acc: 0.6251 - val_loss: 0.3916 - val_acc: 0.0000e+00\n",
      "Epoch 26/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2319 - acc: 0.6251 - val_loss: 0.3964 - val_acc: 0.0000e+00\n",
      "Epoch 27/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2315 - acc: 0.6251 - val_loss: 0.3846 - val_acc: 0.0000e+00\n",
      "Epoch 28/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2311 - acc: 0.6251 - val_loss: 0.3866 - val_acc: 0.0000e+00\n",
      "Epoch 29/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2307 - acc: 0.6251 - val_loss: 0.3849 - val_acc: 0.0000e+00\n",
      "Epoch 30/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2302 - acc: 0.6251 - val_loss: 0.3916 - val_acc: 0.0000e+00\n",
      "Epoch 31/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2297 - acc: 0.6251 - val_loss: 0.3788 - val_acc: 0.0000e+00\n",
      "Epoch 32/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2291 - acc: 0.6251 - val_loss: 0.3776 - val_acc: 0.0000e+00\n",
      "Epoch 33/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2284 - acc: 0.6251 - val_loss: 0.3844 - val_acc: 0.0000e+00\n",
      "Epoch 34/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2276 - acc: 0.6251 - val_loss: 0.3785 - val_acc: 0.0000e+00\n",
      "Epoch 35/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2268 - acc: 0.6251 - val_loss: 0.3808 - val_acc: 0.0000e+00\n",
      "Epoch 36/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2258 - acc: 0.6251 - val_loss: 0.3740 - val_acc: 0.0000e+00\n",
      "Epoch 37/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2248 - acc: 0.6251 - val_loss: 0.3699 - val_acc: 0.0000e+00\n",
      "Epoch 38/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2237 - acc: 0.6251 - val_loss: 0.3740 - val_acc: 0.0000e+00\n",
      "Epoch 39/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2224 - acc: 0.6251 - val_loss: 0.3687 - val_acc: 0.0000e+00\n",
      "Epoch 40/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2210 - acc: 0.6251 - val_loss: 0.3613 - val_acc: 0.0000e+00\n",
      "Epoch 41/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2197 - acc: 0.6251 - val_loss: 0.3651 - val_acc: 0.0000e+00\n",
      "Epoch 42/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2185 - acc: 0.6342 - val_loss: 0.3570 - val_acc: 0.0000e+00\n",
      "Epoch 43/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2168 - acc: 0.6361 - val_loss: 0.3513 - val_acc: 0.2082\n",
      "Epoch 44/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2151 - acc: 0.6701 - val_loss: 0.3481 - val_acc: 0.3407\n",
      "Epoch 45/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2137 - acc: 0.6756 - val_loss: 0.3376 - val_acc: 0.4448\n",
      "Epoch 46/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2124 - acc: 0.6768 - val_loss: 0.3281 - val_acc: 0.4842\n",
      "Epoch 47/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2109 - acc: 0.6796 - val_loss: 0.3242 - val_acc: 0.4937\n",
      "Epoch 48/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2097 - acc: 0.6796 - val_loss: 0.3289 - val_acc: 0.4937\n",
      "Epoch 49/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2083 - acc: 0.6942 - val_loss: 0.3329 - val_acc: 0.4905\n",
      "Epoch 50/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2071 - acc: 0.6961 - val_loss: 0.3216 - val_acc: 0.5032\n",
      "Epoch 51/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2059 - acc: 0.7013 - val_loss: 0.3191 - val_acc: 0.5095\n",
      "Epoch 52/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2048 - acc: 0.7068 - val_loss: 0.3084 - val_acc: 0.5252\n",
      "Epoch 53/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2036 - acc: 0.7096 - val_loss: 0.3104 - val_acc: 0.5237\n",
      "Epoch 54/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2026 - acc: 0.7127 - val_loss: 0.3042 - val_acc: 0.5268\n",
      "Epoch 55/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2016 - acc: 0.7155 - val_loss: 0.3026 - val_acc: 0.5284\n",
      "Epoch 56/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.2005 - acc: 0.7163 - val_loss: 0.2969 - val_acc: 0.5315\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.2000 - acc: 0.7226 - val_loss: 0.3035 - val_acc: 0.5268\n",
      "Epoch 58/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1985 - acc: 0.7253 - val_loss: 0.2923 - val_acc: 0.5331\n",
      "Epoch 59/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1975 - acc: 0.7297 - val_loss: 0.2931 - val_acc: 0.5331\n",
      "Epoch 60/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1964 - acc: 0.7344 - val_loss: 0.2945 - val_acc: 0.5331\n",
      "Epoch 61/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1954 - acc: 0.7364 - val_loss: 0.2886 - val_acc: 0.5363\n",
      "Epoch 62/200\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.1939 - acc: 0.735 - 0s - loss: 0.1944 - acc: 0.7336 - val_loss: 0.2945 - val_acc: 0.5284\n",
      "Epoch 63/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1934 - acc: 0.7380 - val_loss: 0.2824 - val_acc: 0.5410\n",
      "Epoch 64/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1921 - acc: 0.7380 - val_loss: 0.2823 - val_acc: 0.5442\n",
      "Epoch 65/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1907 - acc: 0.7443 - val_loss: 0.2848 - val_acc: 0.5379\n",
      "Epoch 66/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1897 - acc: 0.7455 - val_loss: 0.2909 - val_acc: 0.5379\n",
      "Epoch 67/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1883 - acc: 0.7482 - val_loss: 0.2685 - val_acc: 0.5599\n",
      "Epoch 68/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1867 - acc: 0.7530 - val_loss: 0.2649 - val_acc: 0.5647\n",
      "Epoch 69/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1856 - acc: 0.7502 - val_loss: 0.2661 - val_acc: 0.5662\n",
      "Epoch 70/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1837 - acc: 0.7549 - val_loss: 0.2556 - val_acc: 0.5804\n",
      "Epoch 71/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1820 - acc: 0.7589 - val_loss: 0.2540 - val_acc: 0.5820\n",
      "Epoch 72/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1804 - acc: 0.7676 - val_loss: 0.2442 - val_acc: 0.5931\n",
      "Epoch 73/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1783 - acc: 0.7687 - val_loss: 0.2634 - val_acc: 0.5757\n",
      "Epoch 74/200\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.1763 - acc: 0.774 - 0s - loss: 0.1765 - acc: 0.7739 - val_loss: 0.2416 - val_acc: 0.5978\n",
      "Epoch 75/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1743 - acc: 0.7786 - val_loss: 0.2452 - val_acc: 0.5962\n",
      "Epoch 76/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1719 - acc: 0.7853 - val_loss: 0.2247 - val_acc: 0.6356\n",
      "Epoch 77/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1700 - acc: 0.7865 - val_loss: 0.2176 - val_acc: 0.6672\n",
      "Epoch 78/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1669 - acc: 0.7952 - val_loss: 0.2137 - val_acc: 0.6877\n",
      "Epoch 79/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1642 - acc: 0.7987 - val_loss: 0.2144 - val_acc: 0.6877\n",
      "Epoch 80/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1609 - acc: 0.8039 - val_loss: 0.2120 - val_acc: 0.7050\n",
      "Epoch 81/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1577 - acc: 0.8102 - val_loss: 0.2056 - val_acc: 0.7098\n",
      "Epoch 82/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1547 - acc: 0.8165 - val_loss: 0.2068 - val_acc: 0.7050\n",
      "Epoch 83/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1522 - acc: 0.8216 - val_loss: 0.2011 - val_acc: 0.7050\n",
      "Epoch 84/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1490 - acc: 0.8252 - val_loss: 0.1818 - val_acc: 0.7271\n",
      "Epoch 85/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1466 - acc: 0.8299 - val_loss: 0.1853 - val_acc: 0.7256\n",
      "Epoch 86/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1447 - acc: 0.8303 - val_loss: 0.1833 - val_acc: 0.7303\n",
      "Epoch 87/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1425 - acc: 0.8331 - val_loss: 0.1781 - val_acc: 0.7413\n",
      "Epoch 88/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1407 - acc: 0.8366 - val_loss: 0.1645 - val_acc: 0.7555\n",
      "Epoch 89/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1391 - acc: 0.8374 - val_loss: 0.1738 - val_acc: 0.74760.82\n",
      "Epoch 90/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1374 - acc: 0.8370 - val_loss: 0.1576 - val_acc: 0.7697\n",
      "Epoch 91/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1357 - acc: 0.8398 - val_loss: 0.1637 - val_acc: 0.7697\n",
      "Epoch 92/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1343 - acc: 0.8429 - val_loss: 0.1603 - val_acc: 0.7776\n",
      "Epoch 93/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1327 - acc: 0.8421 - val_loss: 0.1498 - val_acc: 0.7902\n",
      "Epoch 94/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1313 - acc: 0.8461 - val_loss: 0.1541 - val_acc: 0.7902\n",
      "Epoch 95/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1299 - acc: 0.8485 - val_loss: 0.1665 - val_acc: 0.7744\n",
      "Epoch 96/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1287 - acc: 0.8493 - val_loss: 0.1424 - val_acc: 0.8013\n",
      "Epoch 97/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1275 - acc: 0.8540 - val_loss: 0.1506 - val_acc: 0.7934\n",
      "Epoch 98/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1260 - acc: 0.8571 - val_loss: 0.1558 - val_acc: 0.7918\n",
      "Epoch 99/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1245 - acc: 0.8571 - val_loss: 0.1435 - val_acc: 0.8044\n",
      "Epoch 100/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1233 - acc: 0.8579 - val_loss: 0.1548 - val_acc: 0.7965\n",
      "Epoch 101/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1223 - acc: 0.8599 - val_loss: 0.1461 - val_acc: 0.8076\n",
      "Epoch 102/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1208 - acc: 0.8615 - val_loss: 0.1409 - val_acc: 0.8123\n",
      "Epoch 103/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1196 - acc: 0.8627 - val_loss: 0.1457 - val_acc: 0.8091\n",
      "Epoch 104/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1182 - acc: 0.8658 - val_loss: 0.1382 - val_acc: 0.8170\n",
      "Epoch 105/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1172 - acc: 0.8666 - val_loss: 0.1422 - val_acc: 0.8139\n",
      "Epoch 106/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1157 - acc: 0.8674 - val_loss: 0.1270 - val_acc: 0.8328\n",
      "Epoch 107/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1147 - acc: 0.8694 - val_loss: 0.1382 - val_acc: 0.8218\n",
      "Epoch 108/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1136 - acc: 0.8674 - val_loss: 0.1444 - val_acc: 0.8139\n",
      "Epoch 109/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1124 - acc: 0.8713 - val_loss: 0.1361 - val_acc: 0.8265\n",
      "Epoch 110/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1114 - acc: 0.8733 - val_loss: 0.1190 - val_acc: 0.8438\n",
      "Epoch 111/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1103 - acc: 0.8745 - val_loss: 0.1214 - val_acc: 0.8423\n",
      "Epoch 112/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1091 - acc: 0.8745 - val_loss: 0.1171 - val_acc: 0.8454\n",
      "Epoch 113/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1079 - acc: 0.8765 - val_loss: 0.1386 - val_acc: 0.8265\n",
      "Epoch 114/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1072 - acc: 0.8761 - val_loss: 0.1278 - val_acc: 0.8375\n",
      "Epoch 115/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1060 - acc: 0.8788 - val_loss: 0.1131 - val_acc: 0.8502\n",
      "Epoch 116/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1051 - acc: 0.8820 - val_loss: 0.1231 - val_acc: 0.8423\n",
      "Epoch 117/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1040 - acc: 0.8808 - val_loss: 0.1250 - val_acc: 0.8407\n",
      "Epoch 118/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1030 - acc: 0.8820 - val_loss: 0.1088 - val_acc: 0.8612\n",
      "Epoch 119/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1022 - acc: 0.8844 - val_loss: 0.1091 - val_acc: 0.8628\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.1014 - acc: 0.8852 - val_loss: 0.1090 - val_acc: 0.8628\n",
      "Epoch 121/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.1003 - acc: 0.8860 - val_loss: 0.1189 - val_acc: 0.8486\n",
      "Epoch 122/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0995 - acc: 0.8887 - val_loss: 0.1103 - val_acc: 0.8612\n",
      "Epoch 123/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0988 - acc: 0.8891 - val_loss: 0.1015 - val_acc: 0.8675\n",
      "Epoch 124/200\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.0971 - acc: 0.890 - 0s - loss: 0.0979 - acc: 0.8895 - val_loss: 0.0995 - val_acc: 0.8707\n",
      "Epoch 125/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0971 - acc: 0.8911 - val_loss: 0.1061 - val_acc: 0.8628\n",
      "Epoch 126/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0963 - acc: 0.8919 - val_loss: 0.1039 - val_acc: 0.8675\n",
      "Epoch 127/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0955 - acc: 0.8927 - val_loss: 0.1111 - val_acc: 0.8580\n",
      "Epoch 128/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0947 - acc: 0.8931 - val_loss: 0.0976 - val_acc: 0.8801\n",
      "Epoch 129/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0945 - acc: 0.8946 - val_loss: 0.0925 - val_acc: 0.8864\n",
      "Epoch 130/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0935 - acc: 0.8954 - val_loss: 0.1064 - val_acc: 0.8707\n",
      "Epoch 131/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0928 - acc: 0.8954 - val_loss: 0.1041 - val_acc: 0.8754\n",
      "Epoch 132/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0919 - acc: 0.8978 - val_loss: 0.0999 - val_acc: 0.8770\n",
      "Epoch 133/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0914 - acc: 0.8982 - val_loss: 0.0942 - val_acc: 0.8833\n",
      "Epoch 134/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0908 - acc: 0.8986 - val_loss: 0.1023 - val_acc: 0.8770\n",
      "Epoch 135/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0900 - acc: 0.8994 - val_loss: 0.1008 - val_acc: 0.8785\n",
      "Epoch 136/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0895 - acc: 0.8986 - val_loss: 0.1112 - val_acc: 0.8691\n",
      "Epoch 137/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0888 - acc: 0.9009 - val_loss: 0.0935 - val_acc: 0.8817\n",
      "Epoch 138/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0883 - acc: 0.9002 - val_loss: 0.0939 - val_acc: 0.8833\n",
      "Epoch 139/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0877 - acc: 0.9002 - val_loss: 0.0960 - val_acc: 0.8833\n",
      "Epoch 140/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0873 - acc: 0.9017 - val_loss: 0.0927 - val_acc: 0.8880\n",
      "Epoch 141/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0865 - acc: 0.9021 - val_loss: 0.0911 - val_acc: 0.8912\n",
      "Epoch 142/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0861 - acc: 0.9029 - val_loss: 0.0944 - val_acc: 0.8864\n",
      "Epoch 143/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0852 - acc: 0.9029 - val_loss: 0.0818 - val_acc: 0.9006\n",
      "Epoch 144/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0850 - acc: 0.9037 - val_loss: 0.0855 - val_acc: 0.8959\n",
      "Epoch 145/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0845 - acc: 0.9049 - val_loss: 0.0953 - val_acc: 0.8880\n",
      "Epoch 146/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0840 - acc: 0.9045 - val_loss: 0.0962 - val_acc: 0.8864\n",
      "Epoch 147/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0832 - acc: 0.9053 - val_loss: 0.0863 - val_acc: 0.8959\n",
      "Epoch 148/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0826 - acc: 0.9073 - val_loss: 0.0774 - val_acc: 0.9022\n",
      "Epoch 149/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0823 - acc: 0.9065 - val_loss: 0.0890 - val_acc: 0.8927\n",
      "Epoch 150/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0815 - acc: 0.9077 - val_loss: 0.0950 - val_acc: 0.8880\n",
      "Epoch 151/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0814 - acc: 0.9077 - val_loss: 0.0882 - val_acc: 0.8927\n",
      "Epoch 152/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0807 - acc: 0.9061 - val_loss: 0.0887 - val_acc: 0.8943\n",
      "Epoch 153/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0800 - acc: 0.9077 - val_loss: 0.0848 - val_acc: 0.8943\n",
      "Epoch 154/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0797 - acc: 0.9081 - val_loss: 0.0789 - val_acc: 0.9038\n",
      "Epoch 155/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0789 - acc: 0.9104 - val_loss: 0.1020 - val_acc: 0.8738\n",
      "Epoch 156/200\n",
      "2534/2534 [==============================] - ETA: 0s - loss: 0.0803 - acc: 0.905 - 0s - loss: 0.0787 - acc: 0.9092 - val_loss: 0.0862 - val_acc: 0.8943\n",
      "Epoch 157/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0779 - acc: 0.9108 - val_loss: 0.0772 - val_acc: 0.9101\n",
      "Epoch 158/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0775 - acc: 0.9108 - val_loss: 0.0908 - val_acc: 0.8880\n",
      "Epoch 159/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0771 - acc: 0.9108 - val_loss: 0.0776 - val_acc: 0.9085\n",
      "Epoch 160/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0763 - acc: 0.9104 - val_loss: 0.0791 - val_acc: 0.9054\n",
      "Epoch 161/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0758 - acc: 0.9120 - val_loss: 0.0734 - val_acc: 0.9117\n",
      "Epoch 162/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0752 - acc: 0.9136 - val_loss: 0.0852 - val_acc: 0.9006\n",
      "Epoch 163/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0747 - acc: 0.9132 - val_loss: 0.0747 - val_acc: 0.9101\n",
      "Epoch 164/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0742 - acc: 0.9136 - val_loss: 0.0757 - val_acc: 0.9085\n",
      "Epoch 165/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0733 - acc: 0.9136 - val_loss: 0.0780 - val_acc: 0.9054\n",
      "Epoch 166/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0727 - acc: 0.9140 - val_loss: 0.0777 - val_acc: 0.9038\n",
      "Epoch 167/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0721 - acc: 0.9171 - val_loss: 0.0738 - val_acc: 0.9101\n",
      "Epoch 168/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0713 - acc: 0.9171 - val_loss: 0.0781 - val_acc: 0.9038\n",
      "Epoch 169/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0704 - acc: 0.9183 - val_loss: 0.0842 - val_acc: 0.8975\n",
      "Epoch 170/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0699 - acc: 0.9179 - val_loss: 0.0769 - val_acc: 0.9038\n",
      "Epoch 171/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0689 - acc: 0.9191 - val_loss: 0.0752 - val_acc: 0.9054\n",
      "Epoch 172/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0681 - acc: 0.9187 - val_loss: 0.0784 - val_acc: 0.9038\n",
      "Epoch 173/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0671 - acc: 0.9187 - val_loss: 0.0833 - val_acc: 0.8943\n",
      "Epoch 174/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0666 - acc: 0.9191 - val_loss: 0.0792 - val_acc: 0.8975\n",
      "Epoch 175/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0652 - acc: 0.9207 - val_loss: 0.0834 - val_acc: 0.8912\n",
      "Epoch 176/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0645 - acc: 0.9215 - val_loss: 0.0814 - val_acc: 0.8943\n",
      "Epoch 177/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0634 - acc: 0.9230 - val_loss: 0.0759 - val_acc: 0.9054\n",
      "Epoch 178/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0624 - acc: 0.9250 - val_loss: 0.0670 - val_acc: 0.9148\n",
      "Epoch 179/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0611 - acc: 0.9254 - val_loss: 0.0758 - val_acc: 0.9022\n",
      "Epoch 180/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0604 - acc: 0.9258 - val_loss: 0.0676 - val_acc: 0.9132\n",
      "Epoch 181/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0591 - acc: 0.9290 - val_loss: 0.0765 - val_acc: 0.8991\n",
      "Epoch 182/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0583 - acc: 0.9266 - val_loss: 0.0713 - val_acc: 0.9054\n",
      "Epoch 183/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.0573 - acc: 0.9317 - val_loss: 0.0671 - val_acc: 0.9101\n",
      "Epoch 184/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0563 - acc: 0.9309 - val_loss: 0.0706 - val_acc: 0.9054\n",
      "Epoch 185/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0558 - acc: 0.9317 - val_loss: 0.0710 - val_acc: 0.9054\n",
      "Epoch 186/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0546 - acc: 0.9337 - val_loss: 0.0633 - val_acc: 0.9132\n",
      "Epoch 187/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0539 - acc: 0.9345 - val_loss: 0.0668 - val_acc: 0.9085\n",
      "Epoch 188/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0531 - acc: 0.9369 - val_loss: 0.0574 - val_acc: 0.9227\n",
      "Epoch 189/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0523 - acc: 0.9369 - val_loss: 0.0552 - val_acc: 0.9290\n",
      "Epoch 190/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0519 - acc: 0.9361 - val_loss: 0.0689 - val_acc: 0.9022\n",
      "Epoch 191/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0514 - acc: 0.9384 - val_loss: 0.0812 - val_acc: 0.8864\n",
      "Epoch 192/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0506 - acc: 0.9392 - val_loss: 0.0626 - val_acc: 0.9132\n",
      "Epoch 193/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0501 - acc: 0.9408 - val_loss: 0.0595 - val_acc: 0.9227\n",
      "Epoch 194/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0495 - acc: 0.9396 - val_loss: 0.0681 - val_acc: 0.9054\n",
      "Epoch 195/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0489 - acc: 0.9400 - val_loss: 0.0571 - val_acc: 0.9243\n",
      "Epoch 196/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0485 - acc: 0.9396 - val_loss: 0.0601 - val_acc: 0.9164\n",
      "Epoch 197/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0480 - acc: 0.9388 - val_loss: 0.0743 - val_acc: 0.8975\n",
      "Epoch 198/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0481 - acc: 0.9420 - val_loss: 0.0550 - val_acc: 0.9274\n",
      "Epoch 199/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0471 - acc: 0.9420 - val_loss: 0.0534 - val_acc: 0.9322\n",
      "Epoch 200/200\n",
      "2534/2534 [==============================] - 0s - loss: 0.0472 - acc: 0.9424 - val_loss: 0.0528 - val_acc: 0.9385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c88db38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(11, input_shape=(11,), init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(11,init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(11, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=200, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of ~90% reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping all the columns, and incraesing the number of neurons to maximum and epoch value to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(20,), activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2534 samples, validate on 634 samples\n",
      "Epoch 1/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2439 - acc: 0.5699 - val_loss: 0.3442 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2346 - acc: 0.6251 - val_loss: 0.3932 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3977 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2343 - acc: 0.6251 - val_loss: 0.3876 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2341 - acc: 0.6251 - val_loss: 0.3981 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2340 - acc: 0.6251 - val_loss: 0.3883 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2337 - acc: 0.6251 - val_loss: 0.3959 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2334 - acc: 0.6251 - val_loss: 0.3901 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2331 - acc: 0.6251 - val_loss: 0.3966 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2328 - acc: 0.6251 - val_loss: 0.3814 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2324 - acc: 0.6251 - val_loss: 0.3996 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2319 - acc: 0.6251 - val_loss: 0.3895 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2311 - acc: 0.6251 - val_loss: 0.3840 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2302 - acc: 0.6251 - val_loss: 0.3918 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2292 - acc: 0.6251 - val_loss: 0.4072 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2276 - acc: 0.6251 - val_loss: 0.3882 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2255 - acc: 0.6251 - val_loss: 0.3674 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2231 - acc: 0.6251 - val_loss: 0.3707 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2202 - acc: 0.6251 - val_loss: 0.3404 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2166 - acc: 0.6251 - val_loss: 0.3395 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2127 - acc: 0.6290 - val_loss: 0.3593 - val_acc: 0.0142\n",
      "Epoch 22/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2085 - acc: 0.6701 - val_loss: 0.3474 - val_acc: 0.1956\n",
      "Epoch 23/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.2033 - acc: 0.7174 - val_loss: 0.3290 - val_acc: 0.3091\n",
      "Epoch 24/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1984 - acc: 0.7451 - val_loss: 0.3104 - val_acc: 0.4069\n",
      "Epoch 25/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1936 - acc: 0.7553 - val_loss: 0.2872 - val_acc: 0.4700\n",
      "Epoch 26/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1883 - acc: 0.7668 - val_loss: 0.2729 - val_acc: 0.5331\n",
      "Epoch 27/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1838 - acc: 0.7751 - val_loss: 0.2645 - val_acc: 0.5694\n",
      "Epoch 28/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1795 - acc: 0.7814 - val_loss: 0.2746 - val_acc: 0.5473\n",
      "Epoch 29/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1753 - acc: 0.7865 - val_loss: 0.2643 - val_acc: 0.5789\n",
      "Epoch 30/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1714 - acc: 0.7916 - val_loss: 0.2473 - val_acc: 0.6451\n",
      "Epoch 31/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1677 - acc: 0.7956 - val_loss: 0.2190 - val_acc: 0.6940\n",
      "Epoch 32/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1650 - acc: 0.8007 - val_loss: 0.2593 - val_acc: 0.6183\n",
      "Epoch 33/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1618 - acc: 0.8047 - val_loss: 0.2031 - val_acc: 0.7082\n",
      "Epoch 34/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1585 - acc: 0.7999 - val_loss: 0.2210 - val_acc: 0.6940\n",
      "Epoch 35/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1556 - acc: 0.8086 - val_loss: 0.2460 - val_acc: 0.6451\n",
      "Epoch 36/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1528 - acc: 0.8137 - val_loss: 0.2251 - val_acc: 0.6735\n",
      "Epoch 37/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1500 - acc: 0.8157 - val_loss: 0.2142 - val_acc: 0.6956\n",
      "Epoch 38/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1482 - acc: 0.8193 - val_loss: 0.1912 - val_acc: 0.7334\n",
      "Epoch 39/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1459 - acc: 0.8244 - val_loss: 0.1930 - val_acc: 0.7287\n",
      "Epoch 40/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1439 - acc: 0.8197 - val_loss: 0.2033 - val_acc: 0.7161\n",
      "Epoch 41/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1416 - acc: 0.8252 - val_loss: 0.1847 - val_acc: 0.7334\n",
      "Epoch 42/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1398 - acc: 0.8319 - val_loss: 0.2076 - val_acc: 0.7114\n",
      "Epoch 43/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1374 - acc: 0.8331 - val_loss: 0.1686 - val_acc: 0.7618\n",
      "Epoch 44/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1365 - acc: 0.8331 - val_loss: 0.1918 - val_acc: 0.7303\n",
      "Epoch 45/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1345 - acc: 0.8378 - val_loss: 0.1900 - val_acc: 0.7382\n",
      "Epoch 46/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1332 - acc: 0.8378 - val_loss: 0.1669 - val_acc: 0.7634\n",
      "Epoch 47/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1312 - acc: 0.8457 - val_loss: 0.2085 - val_acc: 0.7208\n",
      "Epoch 48/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1303 - acc: 0.8421 - val_loss: 0.1854 - val_acc: 0.7382\n",
      "Epoch 49/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1287 - acc: 0.8461 - val_loss: 0.1746 - val_acc: 0.7571\n",
      "Epoch 50/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1271 - acc: 0.8512 - val_loss: 0.1713 - val_acc: 0.7603\n",
      "Epoch 51/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1265 - acc: 0.8473 - val_loss: 0.1949 - val_acc: 0.7287\n",
      "Epoch 52/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1245 - acc: 0.8500 - val_loss: 0.1714 - val_acc: 0.76180.856\n",
      "Epoch 53/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1238 - acc: 0.8524 - val_loss: 0.2070 - val_acc: 0.7208\n",
      "Epoch 54/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1230 - acc: 0.8532 - val_loss: 0.1775 - val_acc: 0.7539\n",
      "Epoch 55/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1205 - acc: 0.8556 - val_loss: 0.1887 - val_acc: 0.7429\n",
      "Epoch 56/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1202 - acc: 0.8516 - val_loss: 0.1679 - val_acc: 0.7650\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.1190 - acc: 0.8611 - val_loss: 0.1661 - val_acc: 0.7697\n",
      "Epoch 58/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1178 - acc: 0.8611 - val_loss: 0.1747 - val_acc: 0.7571\n",
      "Epoch 59/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1167 - acc: 0.8599 - val_loss: 0.1767 - val_acc: 0.7555\n",
      "Epoch 60/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1157 - acc: 0.8631 - val_loss: 0.1734 - val_acc: 0.7587\n",
      "Epoch 61/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1149 - acc: 0.8635 - val_loss: 0.1592 - val_acc: 0.7713\n",
      "Epoch 62/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1136 - acc: 0.8670 - val_loss: 0.1710 - val_acc: 0.7603\n",
      "Epoch 63/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1120 - acc: 0.8690 - val_loss: 0.1731 - val_acc: 0.7603\n",
      "Epoch 64/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1124 - acc: 0.8654 - val_loss: 0.1541 - val_acc: 0.7839\n",
      "Epoch 65/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1111 - acc: 0.8698 - val_loss: 0.1776 - val_acc: 0.7555\n",
      "Epoch 66/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1096 - acc: 0.8729 - val_loss: 0.1583 - val_acc: 0.7744\n",
      "Epoch 67/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1094 - acc: 0.8745 - val_loss: 0.1469 - val_acc: 0.7950\n",
      "Epoch 68/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1079 - acc: 0.8757 - val_loss: 0.1504 - val_acc: 0.7871\n",
      "Epoch 69/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1066 - acc: 0.8777 - val_loss: 0.1662 - val_acc: 0.7681\n",
      "Epoch 70/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1062 - acc: 0.8769 - val_loss: 0.1693 - val_acc: 0.7603\n",
      "Epoch 71/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1052 - acc: 0.8800 - val_loss: 0.1806 - val_acc: 0.7539\n",
      "Epoch 72/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1047 - acc: 0.8800 - val_loss: 0.1329 - val_acc: 0.8091\n",
      "Epoch 73/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1047 - acc: 0.8812 - val_loss: 0.1498 - val_acc: 0.7902\n",
      "Epoch 74/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1029 - acc: 0.8840 - val_loss: 0.1586 - val_acc: 0.7776\n",
      "Epoch 75/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1014 - acc: 0.8879 - val_loss: 0.1666 - val_acc: 0.7681\n",
      "Epoch 76/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1019 - acc: 0.8871 - val_loss: 0.1575 - val_acc: 0.7792\n",
      "Epoch 77/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1011 - acc: 0.8844 - val_loss: 0.1500 - val_acc: 0.7871\n",
      "Epoch 78/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.1005 - acc: 0.8852 - val_loss: 0.1530 - val_acc: 0.7871\n",
      "Epoch 79/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0987 - acc: 0.8887 - val_loss: 0.1388 - val_acc: 0.8013\n",
      "Epoch 80/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0977 - acc: 0.8927 - val_loss: 0.1506 - val_acc: 0.7871\n",
      "Epoch 81/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0973 - acc: 0.8903 - val_loss: 0.1313 - val_acc: 0.8139\n",
      "Epoch 82/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0969 - acc: 0.8903 - val_loss: 0.1449 - val_acc: 0.7965\n",
      "Epoch 83/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0956 - acc: 0.8938 - val_loss: 0.1210 - val_acc: 0.8360\n",
      "Epoch 84/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0952 - acc: 0.8942 - val_loss: 0.1356 - val_acc: 0.8060\n",
      "Epoch 85/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0945 - acc: 0.8946 - val_loss: 0.1376 - val_acc: 0.8044\n",
      "Epoch 86/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0944 - acc: 0.8934 - val_loss: 0.1304 - val_acc: 0.8186\n",
      "Epoch 87/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0930 - acc: 0.8958 - val_loss: 0.1588 - val_acc: 0.7792\n",
      "Epoch 88/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0925 - acc: 0.8970 - val_loss: 0.1201 - val_acc: 0.8375\n",
      "Epoch 89/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0927 - acc: 0.8970 - val_loss: 0.1238 - val_acc: 0.8312\n",
      "Epoch 90/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0912 - acc: 0.9009 - val_loss: 0.1320 - val_acc: 0.8186\n",
      "Epoch 91/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0916 - acc: 0.8986 - val_loss: 0.1584 - val_acc: 0.7823\n",
      "Epoch 92/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0922 - acc: 0.8954 - val_loss: 0.1245 - val_acc: 0.8344\n",
      "Epoch 93/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0897 - acc: 0.9037 - val_loss: 0.1378 - val_acc: 0.8107\n",
      "Epoch 94/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0892 - acc: 0.9009 - val_loss: 0.1295 - val_acc: 0.8249\n",
      "Epoch 95/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0888 - acc: 0.9045 - val_loss: 0.1257 - val_acc: 0.8328\n",
      "Epoch 96/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0886 - acc: 0.9025 - val_loss: 0.1302 - val_acc: 0.8281\n",
      "Epoch 97/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0879 - acc: 0.9057 - val_loss: 0.1335 - val_acc: 0.8155\n",
      "Epoch 98/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0873 - acc: 0.9053 - val_loss: 0.1223 - val_acc: 0.8375\n",
      "Epoch 99/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0865 - acc: 0.9045 - val_loss: 0.1233 - val_acc: 0.8360\n",
      "Epoch 100/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0862 - acc: 0.9069 - val_loss: 0.1361 - val_acc: 0.8155\n",
      "Epoch 101/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0857 - acc: 0.9069 - val_loss: 0.1301 - val_acc: 0.8297\n",
      "Epoch 102/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0852 - acc: 0.9077 - val_loss: 0.1194 - val_acc: 0.8375\n",
      "Epoch 103/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0848 - acc: 0.9065 - val_loss: 0.1240 - val_acc: 0.8328\n",
      "Epoch 104/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0847 - acc: 0.9081 - val_loss: 0.1380 - val_acc: 0.8202\n",
      "Epoch 105/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0842 - acc: 0.9088 - val_loss: 0.1162 - val_acc: 0.8438\n",
      "Epoch 106/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0842 - acc: 0.9065 - val_loss: 0.1167 - val_acc: 0.8438\n",
      "Epoch 107/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0835 - acc: 0.9100 - val_loss: 0.1320 - val_acc: 0.8233\n",
      "Epoch 108/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0834 - acc: 0.9084 - val_loss: 0.1053 - val_acc: 0.8628\n",
      "Epoch 109/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0825 - acc: 0.9100 - val_loss: 0.1250 - val_acc: 0.8328\n",
      "Epoch 110/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0823 - acc: 0.9108 - val_loss: 0.1170 - val_acc: 0.8438\n",
      "Epoch 111/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0818 - acc: 0.9100 - val_loss: 0.1289 - val_acc: 0.8265\n",
      "Epoch 112/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0825 - acc: 0.9096 - val_loss: 0.1140 - val_acc: 0.8486\n",
      "Epoch 113/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0820 - acc: 0.9084 - val_loss: 0.1170 - val_acc: 0.8423\n",
      "Epoch 114/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0819 - acc: 0.9120 - val_loss: 0.1137 - val_acc: 0.8486\n",
      "Epoch 115/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0815 - acc: 0.9108 - val_loss: 0.1121 - val_acc: 0.8502\n",
      "Epoch 116/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0803 - acc: 0.9116 - val_loss: 0.1337 - val_acc: 0.8233\n",
      "Epoch 117/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0806 - acc: 0.9112 - val_loss: 0.1054 - val_acc: 0.8596\n",
      "Epoch 118/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0797 - acc: 0.9132 - val_loss: 0.1090 - val_acc: 0.8517\n",
      "Epoch 119/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0794 - acc: 0.9132 - val_loss: 0.1092 - val_acc: 0.8565\n",
      "Epoch 120/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0789 - acc: 0.9136 - val_loss: 0.1122 - val_acc: 0.8502\n",
      "Epoch 121/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.0789 - acc: 0.9140 - val_loss: 0.1148 - val_acc: 0.8470\n",
      "Epoch 122/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0789 - acc: 0.9140 - val_loss: 0.0862 - val_acc: 0.8912\n",
      "Epoch 123/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0789 - acc: 0.9124 - val_loss: 0.1187 - val_acc: 0.8407\n",
      "Epoch 124/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0785 - acc: 0.9155 - val_loss: 0.1159 - val_acc: 0.8438\n",
      "Epoch 125/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0787 - acc: 0.9132 - val_loss: 0.1152 - val_acc: 0.8454\n",
      "Epoch 126/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0779 - acc: 0.9128 - val_loss: 0.1172 - val_acc: 0.8423\n",
      "Epoch 127/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0778 - acc: 0.9155 - val_loss: 0.1128 - val_acc: 0.8517\n",
      "Epoch 128/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0767 - acc: 0.9144 - val_loss: 0.1147 - val_acc: 0.8470\n",
      "Epoch 129/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0767 - acc: 0.9155 - val_loss: 0.1038 - val_acc: 0.8644\n",
      "Epoch 130/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0768 - acc: 0.9144 - val_loss: 0.1118 - val_acc: 0.8502\n",
      "Epoch 131/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0770 - acc: 0.9140 - val_loss: 0.0971 - val_acc: 0.8707\n",
      "Epoch 132/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0763 - acc: 0.9171 - val_loss: 0.1046 - val_acc: 0.8612\n",
      "Epoch 133/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0760 - acc: 0.9152 - val_loss: 0.1074 - val_acc: 0.8565\n",
      "Epoch 134/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0764 - acc: 0.9155 - val_loss: 0.1076 - val_acc: 0.8612\n",
      "Epoch 135/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0757 - acc: 0.9179 - val_loss: 0.0968 - val_acc: 0.8738\n",
      "Epoch 136/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0760 - acc: 0.9175 - val_loss: 0.1188 - val_acc: 0.8407\n",
      "Epoch 137/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0751 - acc: 0.9183 - val_loss: 0.1068 - val_acc: 0.8565\n",
      "Epoch 138/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0754 - acc: 0.9175 - val_loss: 0.1032 - val_acc: 0.8628\n",
      "Epoch 139/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0750 - acc: 0.9159 - val_loss: 0.0981 - val_acc: 0.8754\n",
      "Epoch 140/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0753 - acc: 0.9167 - val_loss: 0.0894 - val_acc: 0.8864\n",
      "Epoch 141/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0748 - acc: 0.9179 - val_loss: 0.1030 - val_acc: 0.8644\n",
      "Epoch 142/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0743 - acc: 0.9167 - val_loss: 0.0899 - val_acc: 0.8817\n",
      "Epoch 143/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0739 - acc: 0.9183 - val_loss: 0.1094 - val_acc: 0.8549\n",
      "Epoch 144/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0739 - acc: 0.9187 - val_loss: 0.1029 - val_acc: 0.8675\n",
      "Epoch 145/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0734 - acc: 0.9195 - val_loss: 0.1022 - val_acc: 0.8675\n",
      "Epoch 146/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0735 - acc: 0.9191 - val_loss: 0.1233 - val_acc: 0.8391\n",
      "Epoch 147/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0736 - acc: 0.9199 - val_loss: 0.0862 - val_acc: 0.8912\n",
      "Epoch 148/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0730 - acc: 0.9183 - val_loss: 0.1203 - val_acc: 0.8470\n",
      "Epoch 149/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0723 - acc: 0.9199 - val_loss: 0.0918 - val_acc: 0.8817\n",
      "Epoch 150/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0724 - acc: 0.9215 - val_loss: 0.1012 - val_acc: 0.8644\n",
      "Epoch 151/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0715 - acc: 0.9203 - val_loss: 0.0974 - val_acc: 0.8754\n",
      "Epoch 152/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0715 - acc: 0.9219 - val_loss: 0.1000 - val_acc: 0.8707\n",
      "Epoch 153/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0709 - acc: 0.9223 - val_loss: 0.0945 - val_acc: 0.8801\n",
      "Epoch 154/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0713 - acc: 0.9203 - val_loss: 0.1012 - val_acc: 0.8691\n",
      "Epoch 155/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0707 - acc: 0.9223 - val_loss: 0.0904 - val_acc: 0.8801\n",
      "Epoch 156/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0700 - acc: 0.9227 - val_loss: 0.0916 - val_acc: 0.8817\n",
      "Epoch 157/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0694 - acc: 0.9211 - val_loss: 0.0918 - val_acc: 0.8785\n",
      "Epoch 158/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0690 - acc: 0.9219 - val_loss: 0.0864 - val_acc: 0.8849\n",
      "Epoch 159/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0682 - acc: 0.9219 - val_loss: 0.0964 - val_acc: 0.8707\n",
      "Epoch 160/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0677 - acc: 0.9219 - val_loss: 0.1017 - val_acc: 0.8612\n",
      "Epoch 161/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0668 - acc: 0.9223 - val_loss: 0.1257 - val_acc: 0.8391\n",
      "Epoch 162/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0667 - acc: 0.9219 - val_loss: 0.0780 - val_acc: 0.8912\n",
      "Epoch 163/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0658 - acc: 0.9250 - val_loss: 0.0998 - val_acc: 0.8644\n",
      "Epoch 164/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0651 - acc: 0.9238 - val_loss: 0.0851 - val_acc: 0.8785\n",
      "Epoch 165/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0653 - acc: 0.9203 - val_loss: 0.1018 - val_acc: 0.8580\n",
      "Epoch 166/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0646 - acc: 0.9223 - val_loss: 0.0914 - val_acc: 0.8707\n",
      "Epoch 167/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0635 - acc: 0.9219 - val_loss: 0.0933 - val_acc: 0.8691\n",
      "Epoch 168/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0631 - acc: 0.9234 - val_loss: 0.0903 - val_acc: 0.8707\n",
      "Epoch 169/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0625 - acc: 0.9238 - val_loss: 0.0666 - val_acc: 0.9085\n",
      "Epoch 170/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0621 - acc: 0.9250 - val_loss: 0.1138 - val_acc: 0.8502\n",
      "Epoch 171/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0607 - acc: 0.9270 - val_loss: 0.0715 - val_acc: 0.8927\n",
      "Epoch 172/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0602 - acc: 0.9270 - val_loss: 0.0942 - val_acc: 0.8628\n",
      "Epoch 173/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0596 - acc: 0.9246 - val_loss: 0.0892 - val_acc: 0.8738\n",
      "Epoch 174/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0578 - acc: 0.9270 - val_loss: 0.1060 - val_acc: 0.8533\n",
      "Epoch 175/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0578 - acc: 0.9274 - val_loss: 0.0772 - val_acc: 0.8833\n",
      "Epoch 176/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0574 - acc: 0.9282 - val_loss: 0.0836 - val_acc: 0.8770\n",
      "Epoch 177/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0569 - acc: 0.9294 - val_loss: 0.0662 - val_acc: 0.9038\n",
      "Epoch 178/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0550 - acc: 0.9290 - val_loss: 0.0784 - val_acc: 0.8801\n",
      "Epoch 179/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0559 - acc: 0.9309 - val_loss: 0.1050 - val_acc: 0.85020.9\n",
      "Epoch 180/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0547 - acc: 0.9329 - val_loss: 0.1002 - val_acc: 0.8580\n",
      "Epoch 181/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0535 - acc: 0.9345 - val_loss: 0.0734 - val_acc: 0.8849\n",
      "Epoch 182/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0527 - acc: 0.9329 - val_loss: 0.0987 - val_acc: 0.8565\n",
      "Epoch 183/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0522 - acc: 0.9341 - val_loss: 0.0884 - val_acc: 0.8707\n",
      "Epoch 184/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0518 - acc: 0.9325 - val_loss: 0.0936 - val_acc: 0.8596\n",
      "Epoch 185/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.0510 - acc: 0.9345 - val_loss: 0.0802 - val_acc: 0.8785\n",
      "Epoch 186/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0541 - acc: 0.9298 - val_loss: 0.0775 - val_acc: 0.8817\n",
      "Epoch 187/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0508 - acc: 0.9349 - val_loss: 0.0700 - val_acc: 0.8975\n",
      "Epoch 188/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0493 - acc: 0.9369 - val_loss: 0.0795 - val_acc: 0.8864\n",
      "Epoch 189/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0486 - acc: 0.9384 - val_loss: 0.0800 - val_acc: 0.8817\n",
      "Epoch 190/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0480 - acc: 0.9380 - val_loss: 0.0925 - val_acc: 0.8659\n",
      "Epoch 191/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0481 - acc: 0.9400 - val_loss: 0.0743 - val_acc: 0.8927\n",
      "Epoch 192/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0470 - acc: 0.9412 - val_loss: 0.0867 - val_acc: 0.8785\n",
      "Epoch 193/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0467 - acc: 0.9444 - val_loss: 0.0850 - val_acc: 0.8770\n",
      "Epoch 194/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0467 - acc: 0.9424 - val_loss: 0.0694 - val_acc: 0.9022\n",
      "Epoch 195/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0461 - acc: 0.9444 - val_loss: 0.0938 - val_acc: 0.8691\n",
      "Epoch 196/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0450 - acc: 0.9436 - val_loss: 0.0620 - val_acc: 0.9101\n",
      "Epoch 197/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0453 - acc: 0.9432 - val_loss: 0.0905 - val_acc: 0.87540.945 - ETA: 0s - loss: 0.0446 - acc: 0.944\n",
      "Epoch 198/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0446 - acc: 0.9440 - val_loss: 0.0682 - val_acc: 0.8991\n",
      "Epoch 199/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0446 - acc: 0.9459 - val_loss: 0.0842 - val_acc: 0.8833\n",
      "Epoch 200/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0444 - acc: 0.9444 - val_loss: 0.0742 - val_acc: 0.8975\n",
      "Epoch 201/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0436 - acc: 0.9483 - val_loss: 0.0714 - val_acc: 0.8959\n",
      "Epoch 202/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0425 - acc: 0.9491 - val_loss: 0.0636 - val_acc: 0.9054\n",
      "Epoch 203/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0435 - acc: 0.9467 - val_loss: 0.0566 - val_acc: 0.9101\n",
      "Epoch 204/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0443 - acc: 0.9463 - val_loss: 0.0963 - val_acc: 0.8675\n",
      "Epoch 205/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0424 - acc: 0.9491 - val_loss: 0.0636 - val_acc: 0.9054\n",
      "Epoch 206/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0414 - acc: 0.9499 - val_loss: 0.0569 - val_acc: 0.9196\n",
      "Epoch 207/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0422 - acc: 0.9483 - val_loss: 0.0687 - val_acc: 0.8991\n",
      "Epoch 208/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0405 - acc: 0.9511 - val_loss: 0.0674 - val_acc: 0.8975\n",
      "Epoch 209/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0405 - acc: 0.9495 - val_loss: 0.0621 - val_acc: 0.9022\n",
      "Epoch 210/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0396 - acc: 0.9503 - val_loss: 0.0613 - val_acc: 0.9022\n",
      "Epoch 211/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0411 - acc: 0.9475 - val_loss: 0.0763 - val_acc: 0.8896\n",
      "Epoch 212/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0393 - acc: 0.9491 - val_loss: 0.0531 - val_acc: 0.9259\n",
      "Epoch 213/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0401 - acc: 0.9507 - val_loss: 0.0597 - val_acc: 0.9038\n",
      "Epoch 214/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0384 - acc: 0.9511 - val_loss: 0.0558 - val_acc: 0.9132\n",
      "Epoch 215/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0393 - acc: 0.9511 - val_loss: 0.0722 - val_acc: 0.8912\n",
      "Epoch 216/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0380 - acc: 0.9530 - val_loss: 0.0698 - val_acc: 0.8896\n",
      "Epoch 217/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0380 - acc: 0.9519 - val_loss: 0.0656 - val_acc: 0.8959\n",
      "Epoch 218/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0372 - acc: 0.9558 - val_loss: 0.0545 - val_acc: 0.9180\n",
      "Epoch 219/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0378 - acc: 0.9530 - val_loss: 0.0747 - val_acc: 0.8864\n",
      "Epoch 220/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0371 - acc: 0.9515 - val_loss: 0.0490 - val_acc: 0.9274\n",
      "Epoch 221/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0367 - acc: 0.9526 - val_loss: 0.0648 - val_acc: 0.8991\n",
      "Epoch 222/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0360 - acc: 0.9562 - val_loss: 0.0721 - val_acc: 0.8864\n",
      "Epoch 223/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0355 - acc: 0.9554 - val_loss: 0.0495 - val_acc: 0.9243\n",
      "Epoch 224/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0361 - acc: 0.9554 - val_loss: 0.0529 - val_acc: 0.9148\n",
      "Epoch 225/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0365 - acc: 0.9562 - val_loss: 0.0538 - val_acc: 0.9164\n",
      "Epoch 226/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0362 - acc: 0.9554 - val_loss: 0.0774 - val_acc: 0.8849\n",
      "Epoch 227/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0347 - acc: 0.9578 - val_loss: 0.0496 - val_acc: 0.9243\n",
      "Epoch 228/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0355 - acc: 0.9554 - val_loss: 0.0587 - val_acc: 0.9069\n",
      "Epoch 229/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0358 - acc: 0.9554 - val_loss: 0.0613 - val_acc: 0.9006\n",
      "Epoch 230/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0346 - acc: 0.9566 - val_loss: 0.0538 - val_acc: 0.9180\n",
      "Epoch 231/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0340 - acc: 0.9578 - val_loss: 0.0704 - val_acc: 0.8880\n",
      "Epoch 232/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0343 - acc: 0.9597 - val_loss: 0.0673 - val_acc: 0.8896\n",
      "Epoch 233/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0344 - acc: 0.9586 - val_loss: 0.0608 - val_acc: 0.9022\n",
      "Epoch 234/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0338 - acc: 0.9574 - val_loss: 0.0750 - val_acc: 0.8880\n",
      "Epoch 235/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0336 - acc: 0.9605 - val_loss: 0.0527 - val_acc: 0.9227\n",
      "Epoch 236/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0331 - acc: 0.9570 - val_loss: 0.0684 - val_acc: 0.8896\n",
      "Epoch 237/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0327 - acc: 0.9597 - val_loss: 0.0500 - val_acc: 0.9243\n",
      "Epoch 238/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0326 - acc: 0.9617 - val_loss: 0.0544 - val_acc: 0.9148\n",
      "Epoch 239/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0329 - acc: 0.9605 - val_loss: 0.0384 - val_acc: 0.9464\n",
      "Epoch 240/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0332 - acc: 0.9562 - val_loss: 0.0795 - val_acc: 0.8880\n",
      "Epoch 241/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0332 - acc: 0.9574 - val_loss: 0.0301 - val_acc: 0.9590\n",
      "Epoch 242/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0323 - acc: 0.9601 - val_loss: 0.0474 - val_acc: 0.9306\n",
      "Epoch 243/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0316 - acc: 0.9609 - val_loss: 0.0448 - val_acc: 0.9322\n",
      "Epoch 244/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0321 - acc: 0.9601 - val_loss: 0.0610 - val_acc: 0.9054\n",
      "Epoch 245/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0313 - acc: 0.9633 - val_loss: 0.0377 - val_acc: 0.9495\n",
      "Epoch 246/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0326 - acc: 0.9594 - val_loss: 0.0625 - val_acc: 0.8975\n",
      "Epoch 247/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0320 - acc: 0.9597 - val_loss: 0.0530 - val_acc: 0.9180\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2534/2534 [==============================] - 0s - loss: 0.0306 - acc: 0.9625 - val_loss: 0.0560 - val_acc: 0.9101\n",
      "Epoch 249/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0308 - acc: 0.9625 - val_loss: 0.0521 - val_acc: 0.9180\n",
      "Epoch 250/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0306 - acc: 0.9641 - val_loss: 0.0498 - val_acc: 0.9227\n",
      "Epoch 251/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0299 - acc: 0.9661 - val_loss: 0.0716 - val_acc: 0.8927\n",
      "Epoch 252/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0312 - acc: 0.9605 - val_loss: 0.0644 - val_acc: 0.8991\n",
      "Epoch 253/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0298 - acc: 0.9649 - val_loss: 0.0433 - val_acc: 0.9401\n",
      "Epoch 254/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0305 - acc: 0.9613 - val_loss: 0.0427 - val_acc: 0.9385\n",
      "Epoch 255/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0297 - acc: 0.9613 - val_loss: 0.0566 - val_acc: 0.9117\n",
      "Epoch 256/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0299 - acc: 0.9621 - val_loss: 0.0590 - val_acc: 0.9069\n",
      "Epoch 257/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0293 - acc: 0.9645 - val_loss: 0.0702 - val_acc: 0.8943\n",
      "Epoch 258/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0295 - acc: 0.9633 - val_loss: 0.0550 - val_acc: 0.9117\n",
      "Epoch 259/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0293 - acc: 0.9637 - val_loss: 0.0536 - val_acc: 0.9148\n",
      "Epoch 260/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0299 - acc: 0.9641 - val_loss: 0.0480 - val_acc: 0.9274\n",
      "Epoch 261/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0288 - acc: 0.9645 - val_loss: 0.0710 - val_acc: 0.8943\n",
      "Epoch 262/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0281 - acc: 0.9657 - val_loss: 0.0406 - val_acc: 0.9416\n",
      "Epoch 263/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0286 - acc: 0.9665 - val_loss: 0.0558 - val_acc: 0.9180\n",
      "Epoch 264/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0286 - acc: 0.9661 - val_loss: 0.0408 - val_acc: 0.9432\n",
      "Epoch 265/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0288 - acc: 0.9653 - val_loss: 0.0506 - val_acc: 0.92270.9\n",
      "Epoch 266/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0281 - acc: 0.9641 - val_loss: 0.0404 - val_acc: 0.9401\n",
      "Epoch 267/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0290 - acc: 0.9625 - val_loss: 0.0538 - val_acc: 0.9180\n",
      "Epoch 268/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0295 - acc: 0.9629 - val_loss: 0.0474 - val_acc: 0.9274\n",
      "Epoch 269/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0280 - acc: 0.9661 - val_loss: 0.0339 - val_acc: 0.9495\n",
      "Epoch 270/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0274 - acc: 0.9669 - val_loss: 0.0523 - val_acc: 0.9180\n",
      "Epoch 271/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0285 - acc: 0.9645 - val_loss: 0.0374 - val_acc: 0.9464\n",
      "Epoch 272/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0290 - acc: 0.9641 - val_loss: 0.0342 - val_acc: 0.9495\n",
      "Epoch 273/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0296 - acc: 0.9625 - val_loss: 0.0665 - val_acc: 0.9006\n",
      "Epoch 274/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0275 - acc: 0.9669 - val_loss: 0.0447 - val_acc: 0.9353\n",
      "Epoch 275/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0270 - acc: 0.9669 - val_loss: 0.0343 - val_acc: 0.9495\n",
      "Epoch 276/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0267 - acc: 0.9692 - val_loss: 0.0920 - val_acc: 0.8817\n",
      "Epoch 277/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0294 - acc: 0.9605 - val_loss: 0.0790 - val_acc: 0.8880\n",
      "Epoch 278/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0270 - acc: 0.9676 - val_loss: 0.0358 - val_acc: 0.9479\n",
      "Epoch 279/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0278 - acc: 0.9676 - val_loss: 0.0431 - val_acc: 0.9338\n",
      "Epoch 280/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0279 - acc: 0.9676 - val_loss: 0.0673 - val_acc: 0.9069\n",
      "Epoch 281/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0286 - acc: 0.9669 - val_loss: 0.0786 - val_acc: 0.8864\n",
      "Epoch 282/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0269 - acc: 0.9680 - val_loss: 0.0448 - val_acc: 0.9322\n",
      "Epoch 283/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0283 - acc: 0.9641 - val_loss: 0.0366 - val_acc: 0.9464\n",
      "Epoch 284/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0264 - acc: 0.9665 - val_loss: 0.0405 - val_acc: 0.9385\n",
      "Epoch 285/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0262 - acc: 0.9684 - val_loss: 0.0391 - val_acc: 0.9401\n",
      "Epoch 286/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0259 - acc: 0.9688 - val_loss: 0.0486 - val_acc: 0.9259\n",
      "Epoch 287/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0268 - acc: 0.9661 - val_loss: 0.0560 - val_acc: 0.9164\n",
      "Epoch 288/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0266 - acc: 0.9649 - val_loss: 0.0502 - val_acc: 0.9306\n",
      "Epoch 289/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0263 - acc: 0.9672 - val_loss: 0.0200 - val_acc: 0.9748\n",
      "Epoch 290/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0288 - acc: 0.9657 - val_loss: 0.0338 - val_acc: 0.9511\n",
      "Epoch 291/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0264 - acc: 0.9684 - val_loss: 0.0431 - val_acc: 0.9338\n",
      "Epoch 292/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0257 - acc: 0.9696 - val_loss: 0.0525 - val_acc: 0.9259\n",
      "Epoch 293/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0253 - acc: 0.9692 - val_loss: 0.0449 - val_acc: 0.9322\n",
      "Epoch 294/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0275 - acc: 0.9676 - val_loss: 0.0498 - val_acc: 0.9306\n",
      "Epoch 295/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0258 - acc: 0.9680 - val_loss: 0.0350 - val_acc: 0.9464\n",
      "Epoch 296/500\n",
      "2534/2534 [==============================] - 0s - loss: 0.0251 - acc: 0.9696 - val_loss: 0.0427 - val_acc: 0.9322\n",
      "Epoch 297/500\n",
      "1400/2534 [===============>..............] - ETA: 0s - loss: 0.0269 - acc: 0.9650"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6d63e21c769d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframeX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframeY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtbCallBack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(20,), init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(20,init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(20, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=500, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Accracy of ~95% reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
      "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
      "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
      "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
      "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
      "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
      "\n",
      "          kurt    sp.ent       sfm      mode  centroid   meanfun    minfun  \\\n",
      "0   274.402906  0.893369  0.491918  0.000000  0.059781  0.084279  0.015702   \n",
      "1   634.613855  0.892193  0.513724  0.000000  0.066009  0.107937  0.015826   \n",
      "2  1024.927705  0.846389  0.478905  0.000000  0.077316  0.098706  0.015656   \n",
      "3     4.177296  0.963322  0.727232  0.083878  0.151228  0.088965  0.017798   \n",
      "4     4.333713  0.971955  0.783568  0.104261  0.135120  0.106398  0.016931   \n",
      "\n",
      "     maxfun   meandom    mindom    maxdom   dfrange   modindx  \n",
      "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000  \n",
      "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632  \n",
      "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512  \n",
      "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119  \n",
      "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274  \n"
     ]
    }
   ],
   "source": [
    "dataframeX = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "print(dataframeX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label\n",
      "0  male\n",
      "1  male\n",
      "2  male\n",
      "3  male\n",
      "4  male\n"
     ]
    }
   ],
   "source": [
    "dataframeY = pds.read_csv('/Users/neelambabel/ADS Folder/Assignment 2/voice-data.csv', usecols=[20])\n",
    "print(dataframeY.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      label\n",
      "3163      1\n",
      "3164      1\n",
      "3165      1\n",
      "3166      1\n",
      "3167      1\n"
     ]
    }
   ],
   "source": [
    "def labelToInt(label):\n",
    "    if label == 'male':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "dataframeY.label = dataframeY.label.apply(labelToInt)\n",
    "print(dataframeY.head())\n",
    "print(dataframeY.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelambabel/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(20, input_shape=(20,), activation=\"ReLU\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown activation function:ReLU",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-22670efa42c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/activations.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                     printable_module_name='activation function')\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neelambabel/anaconda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 159\u001b[0;31m                                  ':' + function_name)\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown activation function:ReLU"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1\n",
    "random.seed(datetime.now())\n",
    "\n",
    "# 2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape=(20,), init='uniform', activation='ReLU'))\n",
    "model.add(Dense(20,init='uniform', activation='ReLU'))\n",
    "model.add(Dense(20, init='uniform', activation='ReLU'))\n",
    "model.add(Dense(1, init='uniform', activation='ReLU'))\n",
    "model.summary()\n",
    "\n",
    "# 3\n",
    "import keras\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(dataframeX.values, dataframeY.values, epochs=500, batch_size=50,  verbose=1, validation_split=0.2, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
